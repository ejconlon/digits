% Preamble
% ---
\documentclass{article}

% Packages
% ---
\usepackage{amsmath} % Advanced math typesetting
\usepackage[utf8]{inputenc} % Unicode support (Umlauts etc.)
\usepackage[english]{babel}
\usepackage{hyperref} % Add a link to your document
\usepackage{graphicx} % Add pictures to your document
\usepackage{listings} % Source code formatting and highlighting
\usepackage{csquotes}
\usepackage{svg}
\usepackage{csvsimple}
\usepackage{graphicx}
\usepackage[
backend=biber,
style=alphabetic,
sorting=ynt
]{biblatex}

\addbibresource{report.bib}
\setlength{\parskip}{1em}
\setsvg{svgpath = /Users/charolastra/school/digits/results/}

% Main document
% ---
\begin{document}
% Set up the maketitle command
\author{Eric Conlon}
\title{An Impatient Approach to Classifying Digits with Deep Learning}
\date{2016-12-01}

\maketitle{} % Generates title

\tableofcontents{} % Generates table of contents from sections

\section{Introduction}

Character recognition is a classic computer vision problem with many interesting variants. Even restricting ourselves to 10-digit classification still leaves much room for uncertainty and creativity. The Street View House Numbers dataset (SVHN) \cite{netzer2011reading} poses a such a classification problem that is generally considered to be tougher than the common MNIST dataset \cite{lecun1998mnist}. SVHN is an attractive dataset because it represents a very practical real-world problem and has been discussed at great length.

In this report I explain some simple approaches to classification of both datasets with both conventional and deep learning methods. The conventional baseline here is a Support Vector Classifier using Histogram of Oriented Gradients features, and the deep learning challenger is a Convolutional Neural Network. (The SVC + HOG solution was attempted in the original SVHN paper, and CNN applied later \cite{sermanet2012convolutional}. Both are detailed below.) Given my resource constraints, their results may not rank among the state of the art, but it is my hope that the details of end-to-end architecture and practical engineering considerations will prove to be interesting. I especially want to focus on strategies to learn the most that can be learned in a single sitting at the computer. (Though it seems the longer you can tolerate sitting, the more you'll learn.)

\section{Datasets}

MNIST is a classic labeled dataset consisting of single handwritten digits (0-9) that have been extensively processed. Each digit is centered alone within a 28x28 grid, and all pixel intensities are between to 0 (background) and 255 (foreground). These images do not require common preprocessing techniques such as rescaling, contrast enhancement, or whitening to be immediately useful. There are around 60,000 training examples and 10,000 test examples.

SVHN is a labeled dataset that comes from images of house numbers collected by Google's Street View project. The digits have a wide variety of foreground and background colors, textures, noise, distractions, orientations, and scales. There are two formats provided: the first includes raw images with sets of bounding rectangles, and the second includes a more MNIST-like centered digits. The models here consume only the second format, but I will say more about this classifiers applicability to the first. Overall, this dataset contains almost 8 times more examples than MNIST.

I went with an approximate 60\%-20\%-20\% training-validation-testing split for both datasets (of various total size). For SVHN, I included the ``easier" and much larger \texttt{extra} dataset in the training and validation sets.

\section{Design}

The application follows a standard machine learning pipeline model. See Figure \ref{fig:pipeline} for a visualization of the major stages of this this pipeline. I tried to obey a few guiding principles in its implementation: 

\textbf{It must possible train, test, explore, and evaluate independently.} Too often processing-intensive applications are tightly coupled and require recomputation of intermediate stages. Here, results from most steps are serialized to disk between stages and tagged with their provenance in order to speed up subsequent re-executions of the pipeline from any given stage. Most stages are also executable independently directly from the command line with the desired arguments, and higher-level variants can also be executed.  For example, one can train a model, then later deserialize the trained model and run predictions on another dataset; or one can drive parameter selection over fixed training and validation datasets.

\textbf{Model selection should be an automated part of the training process.} The Tensorflow \cite{tensorflow2015-whitepaper} compatibility layer with SciKit Learn \cite{scikit-learn} presented all sorts of inconsistencies in implementation and documentation, so I had to put together basic parameter search agnostic of either. It's worth noting that many ML engineers serialize a lot of their model parameters and configuration into a rich textual config format (e.g. in the artifacts for \cite{krizhevsky2012imagenet}). For simplicity's sake I did not, but I will the next time around. This might have completed the loop where I had to manually save the best parameters.

\textbf{Common preprocessing, metrics, and visualizations should apply to any model and dataset.} If I were to throw CIFAR-10 \cite{krizhevsky2009learning} at this project, not much would have to change! I would still be able to preprocess, train, test, evaluate, and explore models and predictions the same as MNIST and SVHN. Though the bulk of the application is in a large Python module with unit tests, I also incorporated Jupyter \cite{PER-GRA:2007} notebooks into the workflow to explore results visually.

\begin{figure}[htbp]
  \centering
  \includesvg[height=\textheight,width=\columnwidth]{pipeline}
  \caption{Pipeline stages}
  \label{fig:pipeline}
\end{figure}

\section{Preprocessing and Augmentation}

Minimal preprocessing was required for MNIST. Digits came centered and isolated in images, and all pixel values were quantized to 0 or 1. SVHN, on the other hand, required extensive work. The processing steps for each cropped digit image were:

\begin{enumerate}
  \item Convert to grayscale (to reduce dimensionality)
  \item Crop on the left and right (4 pixels) to reduce distraction
  \item Center on the mean
  \item Perform Global Contrast Normalization (variance scaling over whole image)
  \item Perform Local Contrast Normalization (variance scaling over patches)
\end{enumerate}

Jarrett et al \cite{jarrett2009best} describe the Local Contrast Normalization step, which is more or less a thresholded, Gaussian-weighted variance scaling around each pixel. The results of this preprocessing can be seen in figures to follow. Numpy \cite{van2011numpy}, SciPy \cite{scipy}, and SciKit Image \cite{van2014scikit} were quite helpful in these processing operations.

For smaller datasets, augmentation with small coordinate transformations of data in the preprocessed training set (scaling, translating, and rotating) helped to reduce overfitting and improve accuracy. This was useful for MNIST in particular since the digits were so well cropped and centered that these perturbations looked quite reasonable. The SVHN dataset was not as well cropped, and moreover there was plenty of training data, so it did not benefit from this augmentation. During the repeated trainings of parameter search, it saved a significant amount of time to de/serialize the largest preprocessed SVHN datasets.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{../results/mnist_orig.png}
  \caption{MNIST sample}
  \label{fig:mnist_orig}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{../results/mnist_aug.png}
  \caption{MNIST augmented}
  \label{fig:mnist_aug}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{../results/crop_orig.png}
  \caption{SVHN sample}
  \label{fig:crop_orig}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{../results/crop_proc.png}
  \caption{SVHN processed}
  \label{fig:crop_proc}
\end{figure}

\section{Conventional Learning}

Conventional learning requires that we extract relevant features from our dataset before learning. Sometimes this extraction can be minimal preprocessing (one-hotting, projecting, or rescaling) and sometimes it can be more extensive, especially in higher dimensional datasets. MNIST and SVHN have 784- and 3072-dimensional input spaces, respectively. Even if one were to amass enough data to make learning that size space tractable, one might still suspect that there were more relevant lower-dimensional representations.

One such representation uses Histogram of Oriented Gradients (HOG) as the feature space \cite{netzer2011reading}.  HOG first calculates approximate gradients in both directions for each pixel in the image, then yields a gradient that is the weighted mode of those gradients in a neighborhood around each pixel. The intuition is that changes in value between pixels are more relevant than the pixel values themselves, and that the direction of these change is more useful still.  (Note that aside from RGB to grayscale conversion, the preprocessing mentioned above was not performed here -- local and global mean and variance scaling are essentially irrelevant when taking local derivatives.)

Support Vector Classification with non-linear kernels is a popular and successful method for feature-based learning that is able to draw some pretty exotic class boundaries through input spaces. Using a relatively untuned SVC with RBF kernel over HOG features from MNIST images yielded 73.4\% accuracy on the test set. The same method over a smaller subset of SVHN (\texttt{train} only; no \texttt{extra} data) yielded only 35.3\% accuracy! With the SciKit Learn implementation I couldn't finish a run over the full dataset in any reasonable amount of time, so don't take this result too seriously. (The incremental training approach of neural networks can be quite useful compared to this all-or-nothing training!)

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{../results/mnist_hog.png}
  \caption{MNIST HOG}
  \label{fig:mnist_hog}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{../results/crop_hog.png}
  \caption{SVHN HOG}
  \label{fig:svhn_hog}
\end{figure}

\section{Deep Learning}

In contrast to conventional learning, deep learning with neural networks requires no feature extraction step. Thankfully, this saves the considerable work of developing and applying projections into a useful feature space. (One might think this would make everything easy, but there are still quite a few knobs to turn!) Neural networks represent these features and classify with them using alternating applications of tensor operations, non-linear functions, and more. One learns a useful network as a side effect of minimizing a loss function (a proxy for the distance to a ``perfect" function) with an artful application of differential calculus.

In this case, I used a variant called a Convolutional Neural Network which uses multiple layers and channels of local convolutions, non-linear activations, and pooling (a kind of down-sampling) to approximate patch-based features, followed by multiple fully-connected layers to classify based on those features. Simplified from the recommendations of the original SVHN CNN paper \cite{sermanet2012convolutional} and with extensive parameter selection, a setup with 2 convolution layers (each with width 5, depth 64) and 2 fully-connected layers (size 1024; size 10) seemed to work best. This particular network configuration seemed to be able to memorize a small training set just fine, so it seemed appropriate to try to scale up with these parameters. However, there were a lot of blanks to fill in even after getting that far: How long should it train? How fast should it change? What should it change? (For the full parameter breakdown, see the accompanying artifacts.)

It only complicates matters that training a neural network is not a speedy process. On real-world datasets it can take days or weeks to train with specialized frameworks and appropriate hardware. Many competitive solutions to common classifications problems process their entire dataset more than 10 or 20 times. For this work I had a 5 year old laptop, so I had to be smart about trimming work where I could!

In no particular order, here are some relevant pieces of advice I wish I could have given my past self to maximize accuracy while minimize training time:

\textbf{Use something better than Gradient Descent.} Tensorflow comes with a few sophisticated optimizers that generally work better than simple Gradient Descent. Just switching to the Adam algorithm made a big difference in speed and accuracy.

\textbf{Decrease your learning rate over time.} Tensorflow comes with primitives for exponential learning rate decrease -- use them! Decay rate and factor are two additional parameters to tune but learning is generally pretty robust within an order of magnitude of optimal values.

\textbf{Preprocess carefully.} Mean and variance scaling make a big difference (on the order of tens of percent of accuracy).

\textbf{Initialize carefully.} Xavier initialization is terrible with ReLU and should not be recommended any more. Simple random normal initialization is better.

\textbf{Try inverting your images.} I didn't find any references for this, but including inverted copies of images in the same batch increased accuracy quite a bit. The intuition here is that we are trying to learn how to see dark digits on light backgrounds and vice versa, so we want to train our network to classify based on contrast changes of both kinds (increasing or decreasing intensity). Feeding an image and its inverse in the same batch attempts to train these features equally each optimization step.

\textbf{Regularize to learn.} Regularization is a simple and effective way to prevent overfitting. Do it.

\textbf{Train with equal representation.} Choosing the same number of examples for each class in each batch reduced bias a lot.  It's also relevant to remember the exploration/exploitation tradeoff: Alternating sequential and random batches is a good way to ensure you see the whole dataset, reinforce things you've seen, and insulate you from bad dataset orderings.

\textbf{Use sampling for training and prediction.} If you have a large training or validation set, it's going to be prohibitively slow to train or calculate prediction accuracy every step. (Moreover it's a waste to calculate accuracy every single step, especially when sampling.) Sampling at random from the dataset (especially with equal representation) works well enough.

\textbf{Know when to quit.} It's tough to know a priori how much data you need to see before you can pass judgement on a set of parameters. Instead I set a hard cap on the number of examples I was willing to train on, and also set limit on the number of prediction steps for which the accuracy was not improving. (That is to say I stopped training when it looked like it stopped learning.) For unit testing I trained on smaller datasets and stopped when an artificial accuracy limit was hit.

\textbf{Take out the garbage.} If you're facing memory pressure and swapping too much, try invoking garbage collection at appropriate points (e.g. between training rounds).

\section{Voting}

There is quite a bit of randomness in both the theory and implementation of the deep learning solution.  Practically, I would see results differing by a few percentage points over the same data with different initialization conditions and seeds. A simple technique to reduce this variance was to introduce voting between multiple models: I trained a straight soft-voting ensemble of 10 classifiers (simply averaging the distributions of all ten, then argmaxing to get the predicted class) and saw a final test accuracy several percent higher than the average individual validation accuracy.

\section{Results}

For every model and dataset, the following are calculated and written to disk:

\begin{itemize}
  \item Accuracy
  \item Per-class precision, recall, and F1
  \item Gold and predicted class distributions
  \item Confusion matrix
  \item Most and least certain examples (by entropy)
  \item Convolution weights
  \item Convolution activations for selected examples
  \item Learning curve (loss and accuracy over time)
\end{itemize}

The most relevant metric is probably accuracy on test datasets (shown in Figure \ref{fig:accuracy}). Take it with a grain of salt, though -- as mentioned above, I've traded some accuracy for speed! For an idea of the relative complexity of some of these, it takes about 5 minutes to train a single CNN on MNIST to the given accuracy, and it takes around 10 times that amount of time to train a single CNN on SVHN. The voting ensemble of CNNs is the clear winner for both datasets. For comparison's sake, in one report the SVC + HOG method has been trained to 85.0\% accuracy on SVHN, and the CNN method trained to 94.9\% \cite{sermanet2012convolutional}. To put that into perspective, that CNN saw over 50 times as many examples in training as these did. (Note that those examples were drawn from the same dataset, so one would expect any particular example to be reinforced over 10 times.)  Experiments with larger batch sizes alone improved single CNN performance several points to 70.1\% at the expense of a 6-fold increase in training time, over which period the entire dataset was still only processed twice.

\begin{figure}[htbp]
\centering
\csvautotabular{../results/acc_table.csv}
\caption{Accuracy by model and dataset}
\label{fig:accuracy}
\end{figure}

For the rest of this section, I will focus on single-classifier SVHN CNN performance. The metrics suggest that MNIST is a much simpler dataset; in fact, training was much more forgiving in terms of model and parameter selection.

Per-class metrics like precision, recall, and confusion matrices can be useful when diagnosing class bias or confusion, but in this final analysis they're not very useful, beyond confirming our intuition that sometimes 7's look like 1's, and so on. I found that per-class performance became fairly uniform once I implemented representative sampling.

Some of the most useful visualizations of the training process for neural networks are learning curves, plotting some quantity indicative of learning as a function of experience (in number of examples or optimization steps). Loss and accuracy curves for training and validation sets are shown in Figure \ref{fig:svhn_cnn_lc}. Note that because the CNN seen so little of both sets and because it is sampling, the training and validation curves are very similar. It is easy to see a clear leveling off of the learning process after a surprisingly small number of examples. It could be the case that we are forced into a local minimum due to the relatively small batch size (200 vs 10000 examples) or a relatively high learning rate for the batch size (0.003). A sample that small may not be ``representative" enough of the wide variety of typeography, colors, distractions, and warping that make classifying these images so challenging.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{../logs/tf_crop-huge/learning_curve.png}
  \caption{SVHN CNN learning curve}
  \label{fig:svhn_cnn_lc}
\end{figure}

To get a sense of these challenges, one may want to look carefully at a few examples of the dataset and model output, but which examples are the most interesting? The entropy of an example's label distribution is one way of quantifying that ``interestingness." Low-entropy examples (ones where one label might dominate) are ``easy" for our model. Conversely, high-entropy examples (ones where no label dominates) are ``hard." Figures \ref{fig:svhn_cnn_correct_uncertain_images} and \ref{fig:svhn_cnn_wrong_uncertain_images} show difficult examples a CNN got right and wrong, respectively.  Note that the labels above each display the correct class, the predicted class (the argmax of the distribution of labels), and the predicted class probability. One can start to form tentative hypotheses about how to improve the model: It seems like images predicted correctly have fewer distractions and much less severe offset than those predicted incorrectly. Maybe the addition of distractors or offsets in the augmentation phase could help. There is also evidence of common confusions: it's not surprising that sometimes an 8 with a large lower loop can look like a 0. Maybe training more on oft-confused pairs would reinforce discrimination.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{../logs/tf_crop-huge/test/correct_uncertain_images.png}
  \caption{SVHN CNN least certain correct preditions}
  \label{fig:svhn_cnn_correct_uncertain_images}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{../logs/tf_crop-huge/test/wrong_uncertain_images.png}
  \caption{SVHN CNN least certain incorrect preditions}
  \label{fig:svhn_cnn_wrong_uncertain_images}
\end{figure}

Finally, there are a few visualizations specific to CNNs that can reveal some of their inner workings. Figure \ref{fig:svhn_cnn_weights} shows weights for all channels in the first convolution layer. What's important here is that, at least to the naked eye, the filters appear to be distinct. Figure \ref{fig:svhn_cnn_activations} show the activation of each of these filters on a particular image. They look an awful lot like responses to various directional gradient filters! (Perhaps feeding inverted images in each batch reinforces this edge-sensitivity.)

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{../logs/tf_crop-huge/weights_0.png}
  \caption{SVHN CNN first convolution layer weights}
  \label{fig:svhn_cnn_weights}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{../logs/tf_crop-huge/test/correct_certain_activations_0.png}
  \caption{SVHN CNN activations for a ``7"}
  \label{fig:svhn_cnn_activations}
\end{figure}

\section{The Bottom Line}

CNNs are a great option for image classification tasks like these. They have an uncanny ability to learn representations that are competitive with hand-made features. Moreover, if you've got the data, you can trade more training time for better prediction accuracy with larger batch sizes, more optimization steps, and wider or deeper networks. If you've got specialized computing resources like GPUs, that trade gets even better. However, trying to train first with limited resources is a great way to force you to consider all the details implementing a machine learning system you might otherwise ignore, making that resource trade only more favorable.

\printbibliography[heading=bibintoc,title={References}]

\end{document}